---
title: "Task1"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
df <- read.csv("TeachingRatings.csv");
head(df)
```

## 3. Описание данных
В этом наборе данных есть 13 переменных:

1. X --- идентификатор курса;
1. minority --- принадлежит ли преподаватель к меньшинству;
2. age --- возраст преподавателя;
3. gender --- пол преподавателя;
4. credits --- является ли предмет факультативным;
5. beauty --- оценка физического облика преподавателя группой из шести студентов, усредненная по шести участникам оценки, нормированная;
6. eval --- оценка преподавания курса от 1 (очень плохо) до 5 (отлично);
7. division --- является ли этот курс курсом высшего или низшего отделения (Курсы низшего отделения --- это в основном большие курсы первокурсников и второкурсников);
8. native --- является ли преподаватель носителем английского языка;
9. tenure --- имеет ли преподаватель учёную степень;
10. students --- количетсво студентов, участвовавших в оценке;
11. allstudents --- общее количетсво студентов, обучавшихся на этом курсе;
12. prof --- идентификатор преподавателя;

```{r}
#library(dplyr)
filteredData_s = df[df$prof!=73, ]
filteredData_s <- transform(filteredData_s, allstudents=log(allstudents))
filteredData_s <- transform(filteredData_s, students=log(students))
library(lattice)
panel.hist.splom <- function(x, ...) {
  h <- hist(x, plot = FALSE)
  }
pairs(~age+beauty+allstudents+eval+students,data=filteredData_s)
length(filteredData_s$X)
```

## 4.1 Линейная регрессия
Попытаемся предсказать _beauty_ по _age_ и _students_
Модель: $$y = b_0 + b_1 x_1 + \dots + b_p x_p +\varepsilon$$

```{r}
summary(lm(beauty ~ age, data = filteredData_s)) #Функция, строящая линейную регрессию. Предсказываем beauty по age В полученных результатах смотрим сначала на самую нижнюю строчку, в которой находится p-value, полученное в результате проверки гипотезы о значимости регрессии. Далее смотрим на таблицу Coefficients. В первом столбце показаны коэффициенты свободного члена (b_0) и коэффициенты перед другими предсказывающими признаками (b_1, b_2, etc). Самый правый столбец в данной таблице есть p-value, полученное в результате проверги гипотезы о том, что коэффициент перед признаком равняется нулю.

summary(mdl <- lm(beauty ~ age + students, data = filteredData_s)) #Здесь все то же самое, но уже два предсказывающих признака. В данном случае, p-value проверки гипотезы о том, коэффициент перед moral равняется нулю, есть 0.02. Одна из причин, из-за которой результаты регрессии могут быть неправильными - сильно зависимые "независимые" признаки. В нашем случае moral и hetero не коррелируют, поэтому данный вариант мы отбрасываем. Далее, стоит посмотреть на аутлаеры.

```
Глядя на этот отчёт можно сделать выводы:
1. Гипотеза о незначимости регрессии отвергается (_p-value_ = 2.176e-10)
2. Гипотеза о нулевом коэффициенте при _age_ отвергается (_p-value_ = 1.61e-10)
3. Гипотеза о нулевом коэффициенте при _students_ не отвергается (_p-value_ = 0.452)
```{r}
# Стандартизованные коэффициенты регрессии (так называются коэффициенты регрессии, если признаки сначала стандартизовать, а потом строить регрессию). Их правильнее анализировать, так как их значение не зависит от масштаба.

library(lm.beta)
summary(lm.beta(mdl))
```

```{r}
## Аутлаеры в регрессионной модели
library(MASS)
r <- stdres(mdl) #The standardized residuals. These are normalized to unit variance, fitted including the current data point.
jackknife.res <- studres(mdl) #deleted residuals (аналогично r, но без привязки к текущей точке)
plot(jackknife.res ~ r)
abline(coef = c(0,1))
d1 <- cooks.distance(mdl)
o <- cbind(filteredData_s, d1, r, jackknife.res)
osorted <- o[order(-d1),]
head(osorted)
```


```{r}
o[d1 > 4*mean(d1),] #Один из критериев, по которому можем искать аутлаеры. Смотрим на расстояния Кука у индивидов, которые больше четырех средних расстояний Кука по всем индивидам.
```
С помощью расстояния Кука получается 29 "выбросов". Уитывая, что записей всего 458 это получается 6%. Но попробуем их убрать и проверить модель.


```{r}
#Перестраиваем модель с убранным аутлаером.
summary(mdl2 <- lm(beauty ~ age, data = filteredData_s[!filteredData_s$X %in% o[d1 > 4*mean(d1),]$X,])) 
summary(lm.beta(lm(beauty ~ age, data = filteredData_s[!filteredData_s$X %in% o[d1 > 4*mean(d1),]$X,])))
```
С убранными "выбросами" _p-values_ стали меньше 2e-16, а также возросло значение _Multiple R-squared_
$$R^2 = 1- \displaystyle{{min_{\hat\eta}\mathbf{E}(\eta - \hat\eta)^2}\over{\mathbf{D}\eta}}.$$
```{r}
#Перестраиваем модель с убранным аутлаером.
AIC(mdl)
stepAIC(mdl) #backward
```
Как было показано ранее, значения _students_ модели не нужны, если его убрать AIC уменьшится, но если убрать _age_ --- вырастет.

```{r}
#Теперь посмотрим, как зависимые переменные всё портят. VIF (variance inflation factor) - 1/(1-Rj^2), где Rj - коэффициент детерминации (множественный коэффициент корреляции) j-го предиктора по остальным предикторам.
summary(mdl)
library(car)
vif(mdl)
dfpart.art <- dfpart
set.seed(100)
dfpart.art$hetero2 <- dfpart$hetero + rnorm(nrow(dfpart), 0, 0.1)
summary(mdl.art <- lm(mobility ~ hetero + hetero2 + moral, data = dfpart.art))
vif(mdl.art)
```

